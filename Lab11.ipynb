{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b57628",
   "metadata": {},
   "source": [
    "# Lab 11 Decision Trees\n",
    "This lab is based off the following two repositories [1](https://github.com/zziz/cart) and [2](https://github.com/rasbt/stat451-machine-learning-fs20/blob/master/L06/code/06-trees_demo.ipynb)\n",
    "\n",
    "In this lab we will implement the CART algorithm to train decision trees. This is a greedy heuristic algorithm that does not guarantee optimal decision trees. sklearn has a version of CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree as sktree\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART(object):\n",
    "    def __init__(self, tree = 'cls', criterion = 'gini', prune = 'depth', max_depth = 4, min_criterion = 0.05):\n",
    "        self.feature = None\n",
    "        self.label = None\n",
    "        self.n_samples = None\n",
    "        self.gain = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.threshold = None\n",
    "        self.depth = 0\n",
    "\n",
    "        self.root = None\n",
    "        self.criterion = criterion\n",
    "        self.prune = prune\n",
    "        self.max_depth = max_depth\n",
    "        self.min_criterion = min_criterion\n",
    "        self.tree = tree\n",
    "\n",
    "    def fit(self, features, target):\n",
    "        self.root = CART()\n",
    "        self.root._grow_tree(features, target, self.criterion)\n",
    "        self.root._prune(self.prune, self.max_depth, self.min_criterion, self.root.n_samples)\n",
    "\n",
    "    def predict(self, features):\n",
    "        return np.array([self.root._predict(f) for f in features])\n",
    "\n",
    "    def print_tree(self):\n",
    "        self.root._show_tree(0, ' ')\n",
    "\n",
    "    def _grow_tree(self, features, target, criterion = 'gini'):\n",
    "        self.n_samples = features.shape[0] \n",
    "\n",
    "        if len(np.unique(target)) == 1:\n",
    "            self.label = target[0]\n",
    "            return\n",
    "\n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        #select the most frequently occuring class to be the label\n",
    "        self.label = max([(c, len(target[target == c])) for c in np.unique(target)], key = lambda x : x[1])[0]\n",
    "\n",
    "\n",
    "        impurity_node = self._calc_impurity(criterion, target)\n",
    "        \n",
    "        #evaluate all candidate splits\n",
    "        for col in range(features.shape[1]):\n",
    "            feature_level = np.unique(features[:,col])\n",
    "            #thresholds are the midpoints between the observed feature values\n",
    "            thresholds = (feature_level[:-1] + feature_level[1:]) / 2.0\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                \n",
    "                ############### Fill in #############\n",
    "                #Calculate the target, impurity and p of the left and right child nodes if this candidate split were to be made\n",
    "                target_l = None\n",
    "                impurity_l = None\n",
    "                p_l = None\n",
    "\n",
    "                target_r = None\n",
    "                impurity_r = None\n",
    "                p_r = None\n",
    "\n",
    "                impurity_gain = None \n",
    "                ############### Fill in #############\n",
    "                if impurity_gain > best_gain:\n",
    "                    best_gain = impurity_gain\n",
    "                    best_feature = col\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        self.feature = best_feature\n",
    "        self.gain = best_gain\n",
    "        self.threshold = best_threshold\n",
    "        if best_gain>0:\n",
    "            self._split_tree(features, target, criterion)\n",
    "\n",
    "    def _split_tree(self, features, target, criterion):\n",
    "        features_l = features[features[:, self.feature] <= self.threshold]\n",
    "        target_l = target[features[:, self.feature] <= self.threshold]\n",
    "        self.left = CART()\n",
    "        self.left.depth = self.depth + 1\n",
    "        self.left._grow_tree(features_l, target_l, criterion)\n",
    "\n",
    "        features_r = features[features[:, self.feature] > self.threshold]\n",
    "        target_r = target[features[:, self.feature] > self.threshold]\n",
    "        self.right = CART()\n",
    "        self.right.depth = self.depth + 1\n",
    "        self.right._grow_tree(features_r, target_r, criterion)\n",
    "\n",
    "    def _calc_impurity(self, criterion, target):\n",
    "        if criterion == 'gini':\n",
    "            return 1.0 - sum([(float(len(target[target == c])) / float(target.shape[0])) ** 2.0 for c in np.unique(target)])\n",
    "        else:\n",
    "            entropy = 0.0\n",
    "            for c in np.unique(target):\n",
    "                p = float(len(target[target == c])) / target.shape[0]\n",
    "                if p > 0.0:\n",
    "                    entropy -= p * np.log2(p)\n",
    "            return entropy            \n",
    "\n",
    "    def _prune(self, method, max_depth, min_criterion, n_samples):\n",
    "        if self.feature is None:\n",
    "            return\n",
    "\n",
    "        self.left._prune(method, max_depth, min_criterion, n_samples)\n",
    "        self.right._prune(method, max_depth, min_criterion, n_samples)\n",
    "\n",
    "        pruning = False\n",
    "\n",
    "        if method == 'impurity' and self.left.feature is None and self.right.feature is None: \n",
    "            if (self.gain * float(self.n_samples) / n_samples) < min_criterion:\n",
    "                pruning = True\n",
    "        elif method == 'depth' and self.depth >= max_depth:\n",
    "            pruning = True\n",
    "\n",
    "        if pruning is True:\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.feature = None\n",
    "\n",
    "    def _predict(self, d):\n",
    "        if self.feature != None:\n",
    "            if d[self.feature] <= self.threshold:\n",
    "                return self.left._predict(d)\n",
    "            else:\n",
    "                return self.right._predict(d)\n",
    "        else: \n",
    "            return self.label\n",
    "\n",
    "    def _show_tree(self, depth, cond):\n",
    "        base = '    ' * depth + cond\n",
    "        if self.feature != None:\n",
    "            print(base + 'if X[' + str(self.feature) + '] <= ' + str(self.threshold))\n",
    "            self.left._show_tree(depth+1, 'then ')\n",
    "            self.right._show_tree(depth+1, 'else ')\n",
    "        else:\n",
    "            print(base + '{value: ' + str(self.label) + ', samples: ' + str(self.n_samples) + '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d55a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('\\n\\nClassification Tree With Entropy Criterion')\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "cls = CART(tree = 'cls', criterion = 'entropy', prune = 'depth', max_depth = 3)\n",
    "cls.fit(X_train, y_train)\n",
    "cls.print_tree()\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n",
    "\n",
    "clf = sktree.DecisionTreeClassifier(criterion = 'entropy', max_depth=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "sk_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Sklearn Library Tree Prediction Accuracy:        {}\".format(sum(sk_pred == y_test) / len(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74b132",
   "metadata": {},
   "source": [
    "Using the Gini criterion instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nClassification Tree With Gini Criterion')\n",
    "cls = CART(tree = 'cls', criterion = 'gini', prune = 'depth', max_depth = 3,min_criterion=0)\n",
    "cls.fit(X_train, y_train)\n",
    "#cls.print_tree()\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n",
    "\n",
    "clf = sktree.DecisionTreeClassifier(criterion = 'gini', max_depth=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "sk_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Sklearn Library Tree Prediction Accuracy:        {}\".format(sum(sk_pred == y_test) / len(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31048427",
   "metadata": {},
   "source": [
    "# Visualizing the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[:,[2,3]] #reduce feature set to 2 dimensions for visualizations\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state = 42)\n",
    "\n",
    "cls = CART(tree = 'cls', criterion = 'entropy', prune = 'depth', max_depth = 3)\n",
    "cls.fit(X_train, y_train)\n",
    "cls.print_tree()\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n",
    "\n",
    "clf = sktree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "sk_pred = clf.predict(X_test)\n",
    "print(\"Sklearn Library Tree Prediction Accuracy:        {}\".format(sum(sk_pred == y_test) / len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_decision_regions(X_train, y_train,cls)\n",
    "plt.xlabel('petal length [cm]')\n",
    "plt.ylabel('petal width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.title(\"Custom Decision Tree Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_train, y_train,clf)\n",
    "plt.xlabel('petal length [cm]')\n",
    "plt.ylabel('petal width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.title(\"Sklearn Decision Tree Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6ac23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
